{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tracks.csv', index_col=0, header=[0, 1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = df[df[('set','subset')] == 'small'].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/features.csv', header=[0,1,2], index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = data[data.index.isin(small_data)]\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean memory\n",
    "del data\n",
    "del df\n",
    "del small_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_stats = pd.DataFrame(columns=['num_clusters', 'radius', 'diameter', 'density_r2',\n",
    "                                'density_d2', 'avg_radius', 'avg_diameter', 'avg_density_r2', 'avg_density_d2'])\n",
    "\n",
    "for i in range(8, 17):\n",
    "    # Hierarchical clustering of the dataset for k = 8 to k = 16\n",
    "    clustering = AgglomerativeClustering(n_clusters=i).fit_predict(small_dataset)\n",
    "    clustered = small_dataset.copy()\n",
    "    clustered['cluster'] = clustering\n",
    "    \n",
    "    # Calculating the centroids assuming euclidean distance was used\n",
    "    centroids = clustered.groupby(\"cluster\").mean().values\n",
    "    \n",
    "    radius = {}\n",
    "    density_r2 = {}\n",
    "    density_d2 = {}\n",
    "    diameter = {}\n",
    "    \n",
    "    # Calculate the radius, diameter and density of each cluster \n",
    "    # and add it to a dictionary\n",
    "    for x in range(0, len(centroids)):\n",
    "        points = clustered[clustered[('cluster', '', '')] == x].values.tolist()\n",
    "        # remove the cluster number from the list of points\n",
    "        points = [x.__delitem__(-1) for x in points]\n",
    "        calc = max([np.linalg.norm(centroids[x], point) for point in points])\n",
    "        radius[x] = calc\n",
    "        diameter[x] = radius[x] * 2\n",
    "        density_r2[x] = len(points) / (radius[x] ** 2)\n",
    "        density_d2[x] = len(points) / (diameter[x] ** 2)\n",
    "    \n",
    "    # Calculate avg metrics\n",
    "    avg_radius = np.mean(list(radius.values()))\n",
    "    avg_diameter = np.mean(list(diameter.values()))\n",
    "    avg_density_r2 = np.mean(list(density_r2.values()))\n",
    "    avg_density_d2 = np.mean(list(density_d2.values()))\n",
    "\n",
    "    # Add the metrics of each number of clusters to a dataframe to be easier to compare them\n",
    "    df_row = {'num_clusters': i, 'radius': radius, 'diameter': diameter, 'density_r2': density_r2,\n",
    "            'density_d2': density_d2, 'avg_radius': avg_radius, 'avg_diameter': avg_diameter, \n",
    "            'avg_density_r2': avg_density_r2, 'avg_density_d2': avg_density_d2}\n",
    "    df_row = pd.DataFrame(df_row)\n",
    "    \n",
    "    clustering_stats = pd.concat([clustering_stats, df_row], ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_stats.loc[clustering_stats.num_clusters == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 BRF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/30 21:45:42 WARN Utils: Your hostname, Luiss-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.127 instead (on interface en0)\n",
      "22/05/30 21:45:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/30 21:45:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName=\"Assignment2_E1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCARD_SET = []\n",
    "COMPRESSION_SET = []\n",
    "RETAINED_SET = []\n",
    "summarized_ds = {}\n",
    "clusters_indices = {}\n",
    "num_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar function to read the file in chunks\n",
    "def process_batch(batch):\n",
    "    global i\n",
    "    chunk = np.array(batch)\n",
    "    if i:       \n",
    "        # removing the header\n",
    "        chunk = chunk[4:,:]\n",
    "        chunk = chunk.astype(float)\n",
    "        i = False\n",
    "        start_bfr(chunk)\n",
    "    else:\n",
    "        chunk = chunk.astype(float)\n",
    "        continue_bfr(chunk)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Auxiliar functions\n",
    "def centroid(stats):\n",
    "    # centroid = SUM / N\n",
    "    return stats[1] / stats[0]\n",
    "\n",
    "def variance(stats):\n",
    "    # variance = (SUMSQ / N) - np.square(SUM / N)\n",
    "    return stats[2] / stats[0] - np.square((stats[1] / stats[0]))\n",
    "\n",
    "#std = np.sqrt(variance)\n",
    "\n",
    "def calculate_malahanobis(point, centroid, std_dev):\n",
    "    return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def start_bfr(initial_chunk):\n",
    "    global clusters_indices\n",
    "    global summarized_ds\n",
    "    \n",
    "    # removing the index before the clustering\n",
    "    points = initial_chunk[:,1:]\n",
    "    initial_clusters = AgglomerativeClustering(n_clusters=num_clusters).fit(points)\n",
    "    # indices of the points in each cluster\n",
    "    \n",
    "    # extracting the labels\n",
    "    labels = np.array([[x] for x in initial_clusters.labels_])\n",
    "    points = np.append(points, labels, axis=1)\n",
    "    aux = np.append(initial_chunk, labels, axis=1)\n",
    "    \n",
    "    clusters_indices = {cluster: [x[0] for x in aux if x[-1] == cluster] for cluster in range(initial_clusters.n_clusters_)}\n",
    "    \n",
    "    clusters = {cluster: points[np.where(initial_clusters.labels_ == cluster)][:,:-1]\n",
    "                for cluster in range(initial_clusters.n_clusters_)}\n",
    "    #clusters = {cluster: [x[:-1] for x in points if x[-1] == cluster] for cluster in range(initial_clusters.n_clusters_)}\n",
    "    del aux\n",
    "    gc.collect()\n",
    "    \n",
    "    summarized_ds = calc_ds_stats(clusters)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "def calc_ds_stats(cluster):\n",
    "    summarized_clusters = {}\n",
    "    for cluster_id, points in cluster.items():\n",
    "        N = len(points)\n",
    "        SUM = np.sum(points, axis=0)\n",
    "        SUMSQ = np.sum(np.square(points), axis=0)\n",
    "\n",
    "        summarized_clusters[cluster_id] = (N, SUM, SUMSQ)\n",
    "\n",
    "    return summarized_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 2-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_bfr(chunk):\n",
    "    global DISCARD_SET\n",
    "    global RETAINED_SET\n",
    "    global COMPRESSION_SET\n",
    "    \n",
    "    batch = sc.parallelize(chunk)\n",
    "    DISCARD_SET = batch.map(lambda point: point if\n",
    "                            min([calculate_malahanobis(point[1:], centroid(x[1]), np.sqrt(variance(x[1]), ) ) for x in summarized_ds.items()])\n",
    "                            < (2*np.sqrt(518)) else '') \\\n",
    "                                .filter(lambda point: point != '') \\\n",
    "                                .collect()\n",
    "    \n",
    "    if len(DISCARD_SET) > 0:\n",
    "        RETAINED_SET = np.append(RETAINED_SET, chunk[np.isin(chunk[:, 0], np.array(DISCARD_SET)[:, 0], invert=True)], axis=0)\n",
    "    else:\n",
    "        RETAINED_SET = np.append(RETAINED_SET, chunk, axis=0)\n",
    "    \n",
    "    DISCARD_SET = np.array(DISCARD_SET)\n",
    "    find_cluster(DISCARD_SET)\n",
    "    DISCARD_SET = []\n",
    "    \n",
    "    if RETAINED_SET.size != 0:\n",
    "        process_retained()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 3\n",
    "def find_cluster(set):\n",
    "    global summarized_ds\n",
    "    global clusters_indices\n",
    "    \n",
    "    # iterating through the chunk\n",
    "    for point in set:\n",
    "        min_distance = np.inf\n",
    "\n",
    "        # calculating the minimum distance to a cluster\n",
    "        for cluster, stats in summarized_ds.items():\n",
    "            std = np.square(variance(stats))\n",
    "            distance = calculate_malahanobis(point[1:], centroid(stats), std)\n",
    "            if distance < min_distance:\n",
    "                distance = min_distance\n",
    "                label = cluster\n",
    "                \n",
    "\n",
    "        # saving the point id to the clusters dictionary\n",
    "        # dont need to check distance because we already know it is smaller than 2*sqrt(dimension)\n",
    "        # if it is during finalize() that condition also doesn't apply\n",
    "        clusters_indices[label].append(point[0])\n",
    "            \n",
    "        # updating the statistics\n",
    "        # statistics = (N, SUM, SUMSQ)\n",
    "        # using point[1:] in order to remove the id from the point\n",
    "        statistics = summarized_ds[label]\n",
    "        N = statistics[0] +1\n",
    "        SUM = statistics[1] + point[1:]\n",
    "        SUMSQ = statistics[2] + np.square(point[1:])\n",
    "        summarized_ds.update({label: (N, SUM, SUMSQ)})\n",
    "              \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_retained():\n",
    "    global RETAINED_SET\n",
    "    global COMPRESSION_SET\n",
    "    \n",
    "    X = RETAINED_SET.astype(float)\n",
    "    points = X[:,1:]\n",
    "    # distance threshold = 2x number of dimensions(517)\n",
    "    cs_clusters = AgglomerativeClustering(n_clusters=None, distance_threshold=2*np.sqrt(518)).fit(points)\n",
    "    \n",
    "    labels = np.array([[x] for x in cs_clusters.labels_])\n",
    "    points = np.append(X, labels, axis=1)\n",
    "    clusters_idx = [[x[:-1] for x in points if x[-1] == cluster] for cluster in set(cs_clusters.labels_)]\n",
    "    \n",
    "    COMPRESSION_SET = [x for x in clusters_idx if len(x) > 1]\n",
    "    COMPRESSION_SET = np.array(COMPRESSION_SET)\n",
    "    \n",
    "    idx = [x[i][0] for x in clusters_idx if len(x) > 1 for i in range(len(x))]    \n",
    "    RETAINED_SET = np.delete(RETAINED_SET, np.where(np.isin(RETAINED_SET[:, 0], idx)), axis=0)\n",
    "\n",
    "    process_cs()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cs():\n",
    "    global COMPRESSION_SET\n",
    "    global summarized_ds\n",
    "    global clusters_indices\n",
    "    \n",
    "    for clust in COMPRESSION_SET:\n",
    "        clust = np.array(clust)\n",
    "        c_summary = (len(clust), np.sum(clust[:,1:], axis=0), np.sum(np.square(clust[:,1:]), axis=0))\n",
    "        new_stats = [0,0,0] \n",
    "        var = np.full((518,), np.inf)\n",
    "        \n",
    "        for c, stats in summarized_ds.items():\n",
    "                        \n",
    "            new_stats = [stats[0]+c_summary[0], stats[1]+c_summary[1], stats[2]+c_summary[2]]\n",
    "            n_var = variance(new_stats)\n",
    "            \n",
    "            if all(n_var < var):\n",
    "                var = n_var\n",
    "                label = c\n",
    "                \n",
    "        if all(var < 1.1 * 518):\n",
    "            clusters_indices[label].extend(clust[:,0])\n",
    "            summarized_ds.update({label: (new_stats[0], new_stats[1], new_stats[2])})\n",
    "            COMPRESSION_SET = np.delete(COMPRESSION_SET, np.where(np.isin(COMPRESSION_SET, clust)), axis=0)\n",
    "            \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize():\n",
    "    global summarized_ds\n",
    "    global COMPRESSION_SET\n",
    "    global RETAINED_SET\n",
    "    \n",
    "    final = sc.parallelize(RETAINED_SET)\n",
    "    \n",
    "    final = final.map(lambda point: (min([(calculate_malahanobis(point[1:], centroid(x[1]), np.sqrt(variance(x[1]))), x[0]) for x in summarized_ds.items()], key=lambda i: i[0])[1], {point[0]})) \\\n",
    "                .reduceByKey(lambda a, b: a | b) \\\n",
    "                .map(lambda x: (x[0], list(x[1]))) \\\n",
    "                .collect()\n",
    "    \n",
    "    find_cluster(RETAINED_SET)\n",
    "    \n",
    "    for clust in COMPRESSION_SET:\n",
    "        clust = np.array(clust)\n",
    "        c_summary = (len(clust), np.sum(clust[:, 1:], axis=0), np.sum(np.square(clust[:, 1:]), axis=0))\n",
    "        new_stats = [0, 0, 0]\n",
    "        var = np.full((518,), np.inf)\n",
    "\n",
    "        for c, stats in summarized_ds.items():\n",
    "            new_stats = [stats[0]+c_summary[0], stats[1] + c_summary[1], stats[2]+c_summary[2]]\n",
    "            n_var = variance(new_stats)\n",
    "            if all(n_var < var):\n",
    "                var = n_var\n",
    "                label = c\n",
    "\n",
    "        clusters_indices[label].extend(clust[:, 0])\n",
    "        summarized_ds.update({label: (new_stats[0], new_stats[1], new_stats[2])})\n",
    "    \n",
    "    COMPRESSION_SET = []\n",
    "    RETAINED_SET = []    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/30 21:47:06 WARN TaskSetManager: Stage 1 contains a task of very large size (8361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1971783669.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  COMPRESSION_SET = np.array(COMPRESSION_SET)\n",
      "22/05/30 21:47:36 WARN TaskSetManager: Stage 2 contains a task of very large size (8361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "22/05/30 21:49:47 WARN TaskSetManager: Stage 3 contains a task of very large size (8361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1971783669.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  COMPRESSION_SET = np.array(COMPRESSION_SET)\n",
      "22/05/30 21:54:31 WARN TaskSetManager: Stage 4 contains a task of very large size (8361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1971783669.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  COMPRESSION_SET = np.array(COMPRESSION_SET)\n",
      "22/05/30 22:05:38 WARN TaskSetManager: Stage 5 contains a task of very large size (8361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1971783669.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  COMPRESSION_SET = np.array(COMPRESSION_SET)\n",
      "22/05/30 22:54:20 WARN TaskSetManager: Stage 6 contains a task of very large size (8361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/3618799386.py:10: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n",
      "/var/folders/_l/6yn52l4165j2hbxlxj7ct8640000gn/T/ipykernel_19116/1575245359.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n"
     ]
    }
   ],
   "source": [
    "# Helper function to read the file in chunks\n",
    "DISCARD_SET = []\n",
    "COMPRESSION_SET = []\n",
    "RETAINED_SET = np.empty([0, 519])\n",
    "summarized_ds = {}\n",
    "clusters_indices = {}\n",
    "num_clusters = 8\n",
    "\n",
    "i = True\n",
    "with open('data/features.csv') as f:\n",
    "    batch = []\n",
    "    for line in f:\n",
    "        batch.append(line.rstrip('\\n').split(','))\n",
    "        if len(batch) == 8000:\n",
    "            process_batch(batch)\n",
    "            batch = []\n",
    "if batch:\n",
    "    process_batch(batch)\n",
    "    \n",
    "finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tracks.csv', header=None)\n",
    "a = df[[0,40]].iloc[3:]\n",
    "a = a.rename(columns={0:'id', 40:'genre'})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = clusters_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f77e6bc2d225f99816d788c5a4a60bbea5b0f9a625286da74699d4a3f8b02a8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
