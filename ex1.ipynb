{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/tracks.csv', index_col=0, header=[0, 1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = df[df[('set','subset')] == 'small'].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/features.csv', header=[0,1,2], index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = data[data.index.isin(small_data)]\n",
    "small_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean memory\n",
    "del data\n",
    "del df\n",
    "del small_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_stats = pd.DataFrame(columns=['num_clusters', 'radius', 'diameter', 'density_r2',\n",
    "                                'density_d2', 'avg_radius', 'avg_diameter', 'avg_density_r2', 'avg_density_d2'])\n",
    "\n",
    "for i in range(8, 17):\n",
    "    # Hierarchical clustering of the dataset for k = 8 to k = 16\n",
    "    clustering = AgglomerativeClustering(n_clusters=i).fit_predict(small_dataset)\n",
    "    clustered = small_dataset.copy()\n",
    "    clustered['cluster'] = clustering\n",
    "    \n",
    "    # Calculating the centroids assuming euclidean distance was used\n",
    "    centroids = clustered.groupby(\"cluster\").mean().values\n",
    "    \n",
    "    radius = {}\n",
    "    density_r2 = {}\n",
    "    density_d2 = {}\n",
    "    diameter = {}\n",
    "    \n",
    "    # Calculate the radius, diameter and density of each cluster \n",
    "    # and add it to a dictionary\n",
    "    for x in range(0, len(centroids)):\n",
    "        points = clustered[clustered[('cluster', '', '')] == x].values.tolist()\n",
    "        # remove the cluster number from the list of points\n",
    "        points = [x.__delitem__(-1) for x in points]\n",
    "        calc = max([np.linalg.norm(centroids[x], point) for point in points])\n",
    "        radius[x] = calc\n",
    "        diameter[x] = radius[x] * 2\n",
    "        density_r2[x] = len(points) / (radius[x] ** 2)\n",
    "        density_d2[x] = len(points) / (diameter[x] ** 2)\n",
    "    \n",
    "    # Calculate avg metrics\n",
    "    avg_radius = np.mean(list(radius.values()))\n",
    "    avg_diameter = np.mean(list(diameter.values()))\n",
    "    avg_density_r2 = np.mean(list(density_r2.values()))\n",
    "    avg_density_d2 = np.mean(list(density_d2.values()))\n",
    "\n",
    "    # Add the metrics of each number of clusters to a dataframe to be easier to compare them\n",
    "    df_row = {'num_clusters': i, 'radius': radius, 'diameter': diameter, 'density_r2': density_r2,\n",
    "            'density_d2': density_d2, 'avg_radius': avg_radius, 'avg_diameter': avg_diameter, \n",
    "            'avg_density_r2': avg_density_r2, 'avg_density_d2': avg_density_d2}\n",
    "    df_row = pd.DataFrame(df_row)\n",
    "    \n",
    "    clustering_stats = pd.concat([clustering_stats, df_row], ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_stats.loc[clustering_stats.num_clusters == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 BRF Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 18:03:09 WARN Utils: Your hostname, Luiss-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.127 instead (on interface en0)\n",
      "22/05/29 18:03:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/luismiguel/opt/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/29 18:03:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/29 18:03:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName=\"Assignment2_E1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "def process_batch(batch):\n",
    "    global i\n",
    "    print(batch)\n",
    "    chunk = np.array(batch)\n",
    "    if i == 0:       \n",
    "        #chunk = chunk[4:,:]\n",
    "        #print(chunk)\n",
    "        pass\n",
    "    else:\n",
    "        chunk = sc.parallelize(chunk)\n",
    "        chunk = chunk.map(lambda line: line.split(\",\"))\n",
    "        pass\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read the file in chunks\n",
    "with open('data/features.csv') as f:\n",
    "    batch = []\n",
    "    for line in f:\n",
    "        #line = line.rstrip('\\n').split(',')\n",
    "        batch.append(line.rstrip('\\n').split(','))\n",
    "        if len(batch) == 8000:\n",
    "            process_batch(batch)\n",
    "            batch = []\n",
    "process_batch(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile('data/features.csv')\n",
    "# removing the headers\n",
    "file = file.map(lambda line: line.split(\",\"))\n",
    "file = file.filter(lambda x: x[0].isnumeric())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_sample = file.sample(withReplacement=False, fraction=0.05).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.filter(lambda x: x not in point_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the file into 10 chunks of equal size to emulate batch reading\n",
    "file = file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = np.array(file)\n",
    "file = np.split(file, 10, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCARD_SET = []\n",
    "COMPRESSION_SET = []\n",
    "RETAINED_SET = []\n",
    "clusters_indices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Auxiliar functions\n",
    "def centroid(stats):\n",
    "    # centroid = SUM / N\n",
    "    return stats[1] / stats[0]\n",
    "\n",
    "def variance(stats):\n",
    "    # variance = (SUMSQ / N) - np.square(SUM / N)\n",
    "    return stats[2] / stats[0] - np.square((stats[1] / stats[0]))\n",
    "\n",
    "#std = np.sqrt(variance)\n",
    "\n",
    "def calculate_malahanobis(point, centroid, std_dev):\n",
    "    #print(std_dev)\n",
    "    return np.sqrt(np.sum(np.square((point-centroid)/std_dev)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def start_bfr(initial_chunk, num_clusters):\n",
    "    global clusters_indices\n",
    "    \n",
    "    # removing the index before the clustering\n",
    "    points = initial_chunk[:,1:]\n",
    "    initial_clusters = AgglomerativeClustering(n_clusters=num_clusters).fit(points)\n",
    "    # indices of the points in each cluster\n",
    "    clusters_indices = {cluster: np.where(initial_clusters.labels_ == cluster) for cluster in range(initial_clusters.n_clusters_)}\n",
    "    # extracting the labels\n",
    "    labels = np.array([[x] for x in initial_clusters.labels_])\n",
    "    points = np.append(points, labels, axis=1)\n",
    "    \n",
    "    clusters = {cluster: [x[:-1] for x in points if x[-1] == cluster] for cluster in set(initial_clusters.labels_)}\n",
    "    \n",
    "    summarized_ds = calc_ds_stats(clusters)\n",
    "    \n",
    "    return summarized_ds\n",
    "\n",
    "\n",
    "def calc_ds_stats(cluster):\n",
    "    summarized_clusters = {}\n",
    "    for cluster_id, points in cluster.items():\n",
    "        N = len(points)\n",
    "        SUM = np.sum(points, axis=0)\n",
    "        SUMSQ = np.sum(np.square(points), axis=0)\n",
    "\n",
    "        summarized_clusters[cluster_id] = (N, SUM, SUMSQ)\n",
    "\n",
    "    return summarized_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = small_dataset.sample(frac=0.1, replace=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = small_dataset[~small_dataset.isin(b)].dropna()\n",
    "small_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = small_dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = np.array(small_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_ds = start_bfr(np.array(small_dataset), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for point in b:\n",
    "    #c = [u for u in summarized_ds.values()]\n",
    "    e = min([(calculate_malahanobis(point[1:], centroid(x[1]), np.sqrt(variance(x[1]))),x[0]) for x in summarized_ds.items()], \n",
    "            key= lambda x: np.isnan(x[0]))\n",
    "    #a = min([calculate_malahanobis(point[1:], centroid(x), np.sqrt(variance(x))) for x in summarized_ds.values]) < 2 * np.sqrt(variance(x))\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ''\n",
    "for chunk in file:\n",
    "    \n",
    "    DISCARD_SET = chunk.filter(lambda point: (point, x) for point, x in \n",
    "                               min([calculate_malahanobis(point[1:], centroid(x), np.sqrt(variance(x))) for x in summarized_ds]) < 2*np.sqrt(variance(x))\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3\n",
    "def process_chunk(chunk):\n",
    "    #global summarized_ds\n",
    "    #global clusters_indices\n",
    "    \n",
    "    # iterating through the chunk\n",
    "    for point in chunk:\n",
    "        min_distance = np.inf\n",
    "\n",
    "        # calculating the minimum distance to a cluster\n",
    "        for cluster, stats in summarized_ds:\n",
    "            std = np.square(variance(stats))\n",
    "            distance = calculate_malahanobis(point[1:], centroid(stats), std)\n",
    "            if distance < min_distance:\n",
    "                distance = min_distance\n",
    "                label = cluster\n",
    "                \n",
    "        # checking if it is closer than the threshold: 2x STD\n",
    "        if min_distance < 2 * std:\n",
    "            # saving the point id to the clusters dictionary\n",
    "            clusters_indices[label].append(point[0])\n",
    "            \n",
    "            # updating the statistics\n",
    "            # statistics = (N, SUM, SUMSQ)\n",
    "            # using point[1:] in order to remove the id from the point\n",
    "            statistics = summarized_ds[label]\n",
    "            N = statistics[0] +1\n",
    "            SUM = statistics[1] + point[1:]\n",
    "            SUMSQ = statistics[2] + np.square(point[1:])\n",
    "            summarized_ds.update({label: (N, SUM, SUMSQ)})\n",
    "        \n",
    "        else:\n",
    "            RETAINED_SET.append(point)   \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_retained():\n",
    "    # distance threshold = 2x number of dimensions(517)\n",
    "    cs_clusters = AgglomerativeClustering(n_clusters=None, distance_threshold=2*np.sqrt(518)).fit(RETAINED_SET)\n",
    "    \n",
    "    labels = np.array([[x] for x in cs_clusters.labels_])\n",
    "    points = np.append(RETAINED_SET, labels, axis=1)\n",
    "    clusters_idx = {cluster: [x[0] for x in points if x[-1] == cluster] for cluster in set(cs_clusters.labels_)}\n",
    "    clusters = {cluster: [x[1:-1] for x in points if x[-1] == cluster] for cluster in set(cs_clusters.labels_)}\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cs_stats(grouping):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cs():\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f77e6bc2d225f99816d788c5a4a60bbea5b0f9a625286da74699d4a3f8b02a8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
